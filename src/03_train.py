# src/03_train_enhanced.py
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os

def train_enhanced_model():
    print("ü§ñ –û–ë–£–ß–ê–ï–ú ML –° 10 –ö–ê–¢–ï–ì–û–†–ò–Ø–ú–ò...")
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
    print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏...")
    features_df = pd.read_parquet('user_features_enhanced.pq')
    
    print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:")
    print(f"- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(features_df)}")
    print(f"- –§–∏—á–µ–π: {len(features_df.columns)}")
    print(f"- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
    print(features_df['target_product'].value_counts())
    
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüîß –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    X = features_df.drop(['user_id', 'target_product'], axis=1, errors='ignore')
    y = features_df['target_product']
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    X = X.fillna(0)
    
    print(f"üìà –§–∏—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X.columns)}")
    
    # 3. –ö–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (10 –∫–ª–∞—Å—Å–æ–≤!)
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    print(f"üéØ –ö–ª–∞—Å—Å—ã ({len(label_encoder.classes_)}): {label_encoder.classes_}")
    
    # 4. –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –∫–ª–∞—Å—Å—ã (–≤–∞–∂–Ω–æ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
    class_weights = class_weight.compute_sample_weight(
        'balanced',
        y_encoded
    )
    
    # 5. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"üìö –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}")
    print(f"üß™ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)}")
    
    # 6. –û–±—É—á–∞–µ–º XGBoost —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    print("\nüöÄ –û–±—É—á–∞–µ–º XGBoost –¥–ª—è 10 –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
    
    model = xgb.XGBClassifier(
        n_estimators=150,           # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        max_depth=8,                # –ì–ª—É–±–∂–µ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        learning_rate=0.1,
        random_state=42,
        eval_metric='mlogloss',     # –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        verbosity=1,
        scale_pos_weight=1,
        subsample=0.8,
        colsample_bytree=0.8
    )
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=10,
        sample_weight=class_weights[:len(X_train)]  # –í–µ—Å–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    )
    
    # 7. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    print("\nüìä –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò (10 –ö–ê–¢–ï–ì–û–†–ò–ô):")
    
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2%}")
    print(f"üìà Classification Report:")
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
    
    # 8. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - 10 –∫–∞—Ç–µ–≥–æ—Ä–∏–π')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ')
    plt.ylabel('–ò—Å—Ç–∏–Ω–∞')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig('confusion_matrix_10_classes.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("üíæ –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ confusion_matrix_10_classes.png")
    
    # 9. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüîù –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í:")
    feature_importance = model.feature_importances_
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print(importance_df.head(15))
    
    # 10. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
    os.makedirs('models', exist_ok=True)
    
    model.save_model('models/xgboost_model_enhanced.json')
    
    with open('models/label_encoder_enhanced.pkl', 'wb') as f:
        pickle.dump(label_encoder, f)
    
    with open('models/feature_names_enhanced.pkl', 'wb') as f:
        pickle.dump(X.columns.tolist(), f)
    
    # 11. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("\nüëÄ –ü–†–ò–ú–ï–†–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô (10 –ö–ê–¢–ï–ì–û–†–ò–ô):")
    sample_indices = np.random.choice(len(X_test), 8, replace=False)
    
    for idx in sample_indices:
        user_features = X_test.iloc[idx:idx+1]
        true_label = y_test[idx]
        pred_label = y_pred[idx]
        
        true_product = label_encoder.inverse_transform([true_label])[0]
        pred_product = label_encoder.inverse_transform([pred_label])[0]
        
        status = "‚úÖ" if true_product == pred_product else "‚ùå"
        print(f"{status} –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {idx}: –ò—Å—Ç–∏–Ω–∞ = {true_product:20} –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ = {pred_product}")
    
    print(f"\nüéâ –ú–û–î–ï–õ–¨ –° 10 –ö–ê–¢–ï–ì–û–†–ò–Ø–ú–ò –û–ë–£–ß–ï–ù–ê!")
    print(f"üìä –†–µ–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2%}")

if __name__ == "__main__":
    train_enhanced_model()