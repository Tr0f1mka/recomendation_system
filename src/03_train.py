# src/03_train.py
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os

def train_model():
    print("ü§ñ –û–ë–£–ß–ê–ï–ú ML-–ú–û–î–ï–õ–¨...")
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∏—á–∏
    print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∏—á–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
    features_df = pd.read_parquet('user_features.pq')
    
    print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:")
    print(f"- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(features_df)}")
    print(f"- –§–∏—á–µ–π: {len(features_df.columns)}")
    print(f"- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
    print(features_df['target_product'].value_counts())
    
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüîß –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    
    # –£–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    X = features_df.drop(['user_id', 'target_product'], axis=1, errors='ignore')
    y = features_df['target_product']
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    X = X.fillna(0)
    
    print(f"üìà –§–∏—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {X.columns.tolist()}")
    
    # 3. –ö–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    print(f"üéØ –ö–ª–∞—Å—Å—ã: {label_encoder.classes_}")
    
    # 4. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"üìö –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}")
    print(f"üß™ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)}")
    
    # 5. –û–±—É—á–∞–µ–º XGBoost
    print("\nüöÄ –û–±—É—á–∞–µ–º XGBoost –º–æ–¥–µ–ª—å...")
    
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        eval_metric='mlogloss',
        verbosity=1
    )
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=True
    )
    
    # 6. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    print("\nüìä –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò:")
    
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2%}")
    print(f"üìà Classification Report:")
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
    
    # 7. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüîù –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í:")
    feature_importance = model.feature_importances_
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print(importance_df.head(10))
    
    # 8. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plt.figure(figsize=(10, 6))
    sns.barplot(data=importance_df.head(10), x='importance', y='feature')
    plt.title('–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("üíæ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ feature_importance.png")
    
    # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ—Ç
    os.makedirs('models', exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model.save_model('models/xgboost_model.json')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º label encoder
    with open('models/label_encoder.pkl', 'wb') as f:
        pickle.dump(label_encoder, f)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π
    with open('models/feature_names.pkl', 'wb') as f:
        pickle.dump(X.columns.tolist(), f)
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")
    
    # 10. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("\nüëÄ –ü–†–ò–ú–ï–†–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
    sample_indices = np.random.choice(len(X_test), 100, replace=False)
    
    for idx in sample_indices:
        user_features = X_test.iloc[idx:idx+1]
        true_label = y_test[idx]
        pred_label = y_pred[idx]
        
        true_product = label_encoder.inverse_transform([true_label])[0]
        pred_product = label_encoder.inverse_transform([pred_label])[0]
        
        print(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {idx}: –ò—Å—Ç–∏–Ω–∞ = {true_product}, –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ = {pred_product}")
    
    return model, label_encoder

def analyze_model_performance(model, X_test, y_test, label_encoder):
    """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏"""
    print("\nüìà –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó:")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    y_pred_proba = model.predict_proba(X_test)
    
    # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
    confidence = np.max(y_pred_proba, axis=1).mean()
    print(f"ü§î –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {confidence:.2%}")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    confidence_distribution = np.max(y_pred_proba, axis=1)
    print(f"üìä –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_distribution.min():.2%}")
    print(f"üìä –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_distribution.max():.2%}")

if __name__ == "__main__":
    model, label_encoder = train_model()
    
    print(f"\nüéâ –ú–û–î–ï–õ–¨ –û–ë–£–ß–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ models/")
    print(f"üéØ –ì–æ—Ç–æ–≤–æ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –¥–µ–º–æ-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞!")